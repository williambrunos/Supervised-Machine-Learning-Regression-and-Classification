{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/williambrunos/Supervised-Machine-Learning-Regression-and-Classification/blob/main/Week_3/classification_with_lreg/classification_with_lreg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePF70hFFCiik"
      },
      "source": [
        "# Classification with Logistic Regression\n",
        "\n",
        "## Motivations\n",
        "\n",
        "Classification models tends to predict a target $y$ wich can assume only one value in a certain range of well defined values, not any number in a infinite range of numbers.\n",
        "\n",
        "On this notebook, class and category are going to be used to refer to the same thing.\n",
        "\n",
        "Linear regression isn't the best model to perform a classification:\n",
        "\n",
        "- It has predictions in a range of infinite numbers, not only in a certain and specific range of discrete values.\n",
        "- If this problem is solved using linear regression, a certain outlier can produce predictions that misclassifies the data, modifying a lot the **decision boundary** of the algorithm.\n",
        "\n",
        "## Logistic Regression\n",
        "\n",
        "Logistic regression algorithm fits a 'S' shaped curve into the data, being a good algorithm for **binary classifications**. The function curve fitted to the data is called the **sigmoid function**.\n",
        "\n",
        "$$g(z) = \\frac{1}{1 + e^{-z}}, 0 < g(z) < 1$$\n",
        "\n",
        "![sigmoid function](https://qph.cf2.quoracdn.net/main-qimg-07066668c05a556f1ff25040414a32b7)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression Algorithm\n",
        "\n",
        "- First step:\n",
        "\n",
        "$$f_{\\vec w, b}(\\vec x) = \\vec w \\bullet \\vec x + b â‡’ z = \\vec w \\bullet \\vec x + b$$\n",
        "\n",
        "- Second step: calculate the sigmoid function using $z$:\n",
        "\n",
        "$$g(z) = \\frac{1}{1 + e^{-z}}$$\n",
        "\n",
        "So, to summarize, the lreg model $f$ of $x$ is:\n",
        "\n",
        "$$f_{\\vec w, b}(\\vec x) = g(\\vec w \\bullet \\vec x + b) = \\frac{1}{1 + e^{-(\\vec w \\bullet \\vec x + b)}}$$\n",
        "\n",
        "And what f does is: receives a data X as input and predicts a certain value between 0 and 1. How to interpret this output value of the model?\n",
        "\n",
        "**Note**: the model function does not have to be a linear model!\n",
        "\n",
        "Well, one way of doing this is consideering the output as \"the probability of the prediction for that data being equal to 1/positive/presence\".\n",
        "\n",
        "$$P(y=0) + P(y=1) = 1$$\n",
        "$$f(\\vec x) = P(y = 1|\\vec x, \\vec w, b)$$"
      ],
      "metadata": {
        "id": "wCNKnC_1KAmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BhPBrDVwJH16"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "classification_with_lreg.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}