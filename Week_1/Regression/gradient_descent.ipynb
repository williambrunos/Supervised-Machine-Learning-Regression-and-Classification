{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gradient_descent.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/williambrunos/Supervised-Machine-Learning-Regression-and-Classification/blob/main/Week_1/Regression/gradient_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model with gradient descent \n",
        "\n",
        "In the last class (**regression_model**), we learnt about how the model can adjust the parameters w and b in order to improve the accuracy of the predictions minimizing the cost function $J(w, b)$. The main technique used to do so is called the **gradient descent** method. \n",
        "\n",
        "**obs**: it's imnportant to notice that we are working with a cost function which values depends only of two paramters and we are going to use the gradient descent algorithm. Turns out that we can apply this algorithm to minimize any function according to its parameters, for instance: $J(w_0, w_1, w_2...w_n, b)$\n",
        "\n",
        "Outline:\n",
        "1. Start with random values for w and b;\n",
        "2. Keep changing w and b in order to reduce J(w, b);\n",
        "3. Until we settle at or near a minimun;\n",
        "\n",
        "We have to take care of the local minimas!\n",
        "\n",
        "Beggining with random values for w and b, the algorithm looks foward for the steepest descent on the cost function surface in the features space and tooks a baby step on that direction. The direction taken by the algorithm at some point is the opposite of that indicated by the gradient of the function at that same point.\n",
        "\n",
        "Note: The gradient of the function at some point is a vector which indicates the direction of the highest growing of the function.\n",
        "\n",
        "Take a look at these images to visualize better this algorithm:\n",
        "\n",
        "$$figure 1 - 2D gradient descent$$\n",
        "![2D gradient descent](https://miro.medium.com/max/1142/1*AZzu43KoxDamVpWMVW0zfw.png)\n",
        "$$refference - Mediun$$\n",
        "\n",
        "$$figure 2 - 3D gradient descent$$\n",
        "![3D gradient descent](https://miro.medium.com/proxy/1*f9a162GhpMbiTVTAua_lLQ.png)\n",
        "$$refference - Mediun$$\n"
      ],
      "metadata": {
        "id": "z8EeGlt8Teoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing Gradient Descent\n",
        "\n",
        "Repeat these steps until convergence:\n",
        "\n",
        "$$w_{i+1} = w_i - \\alpha \\frac{\\partial}{\\partial{w}} J(w_i, b_i)$$\n",
        "$$b_{i+1} = b_i - \\alpha \\frac{\\partial}{\\partial{w}} J(w_i, b_i)$$\n",
        "\n",
        "- $\\alpha \\in [0, 1]$: **learning rate**, controls how big is going to be every step at each iteration of the algorithm;\n",
        "- $\\frac{\\partial}{\\partial{w}} J(w_i, b_i)$: sets the direction of the steepest step taken by the algorithm;\n",
        "\n",
        "**Note**: The updates of w and b must be **SIMULTANEOUS**"
      ],
      "metadata": {
        "id": "hkP-vOuPewhE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent Intuition\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "CkBmDffvkBya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent for Linear Regression\n",
        "\n",
        "In this section, we are going to see the implementation of the mean squared error cost function into a linear regression gradient descent algorithm.\n",
        "\n",
        "Linear regression model: $f_{w, b}(x) = wx + b$\n",
        "\n",
        "Cost function: $J(w, b) = \\frac{1}{2m}\\sum_{i=1}^{m}(f_{w, b}(x^{(i)} - y^{(i)})^2$\n",
        "\n",
        "With these two functions defined, we can start finding the update formula for $w_{i+1}$ using the gradient descent and a little bit of calculus:\n",
        "\n",
        "$$\\frac{\\partial}{\\partial{w}}J(w, b) = \\frac{\\partial}{\\partial{w}}[\\frac{1}{2m}\\sum_{i=1}^{m} (f_{w, b}(x^{(i)} - y^{(i)})^2]$$\n",
        "\n",
        "Let's change the $f_{w, b}(x^{(i)})$ for $wx^{(i)} + b$ on the equation above according to our definitions:\n",
        "\n",
        "$$\\frac{\\partial}{\\partial{w}}J(w, b) = \\frac{\\partial}{\\partial{w}}[\\frac{1}{2m}\\sum_{i=1}^{m} (f_{w, b}(x^{(i)} - y^{(i)})^2] ⟺$$\n",
        "\n",
        "$$\\frac{\\partial}{\\partial{w}}J(w, b) = \\frac{\\partial}{\\partial{w}}[\\frac{1}{2m}\\sum_{i=1}^{m} (wx^{(i)} + b - y^{(i)})^2] ⟺$$\n",
        "\n",
        "$$\\frac{\\partial}{\\partial{w}}J(w, b) = \\frac{2}{2m} \\sum_{i=1}^{m}(wx^{(i)} + b - y^{(i)})x^{(i)} ⟺$$\n",
        "\n",
        "$$\\frac{\\partial}{\\partial{w}}J(w, b) = \\frac{1}{m} \\sum_{i=1}^{m}(wx^{(i)} + b - y^{(i)})x^{(i)} ⟺$$\n",
        "\n",
        "$$\\frac{\\partial}{\\partial{w}}J(w, b) = \\frac{1}{m} \\sum_{i=1}^{m}(f_{w, b}(x^{(i)}) - y^{(i)})x^{(i)} \\;\\;\\;\\;□$$\n",
        "\n",
        "The last equation above is the partial derivative of J in w on the linear regression.\n",
        "\n",
        "We repeat the same derivative process upon the J function but now in terms of b:\n",
        "\n",
        "$$\\frac{\\partial}{\\partial{b}}J(w, b) = \\frac{\\partial}{\\partial{b}}[\\frac{1}{2m}\\sum_{i=1}^{m} (f_{w, b}(x^{(i)} - y^{(i)})^2]$$\n",
        "\n",
        "Let's change the $f_{w, b}(x^{(i)})$ for $wx^{(i)} + b$ on the equation above according to our definitions:\n",
        "\n",
        "$$\\frac{\\partial}{\\partial{b}}J(w, b) = \\frac{\\partial}{\\partial{b}}[\\frac{1}{2m}\\sum_{i=1}^{m} (f_{w, b}(x^{(i)} - y^{(i)})^2] ⟺$$\n",
        "\n",
        "$$\\frac{\\partial}{\\partial{b}}J(w, b) = \\frac{\\partial}{\\partial{b}}[\\frac{1}{2m}\\sum_{i=1}^{m} (wx^{(i)} + b - y^{(i)})^2] ⟺$$\n",
        "\n",
        "$$\\frac{\\partial}{\\partial{b}}J(w, b) = \\frac{2}{2m} \\sum_{i=1}^{m}(wx^{(i)} + b - y^{(i)})1 ⟺$$\n",
        "\n",
        "$$\\frac{\\partial}{\\partial{b}}J(w, b) = \\frac{1}{m} \\sum_{i=1}^{m}(f_{w, b}(x^{(i)}) - y^{(i)}) \\;\\;\\;\\;□$$\n",
        "\n",
        "he last equation above is the partial derivative of J in b on the linear regression.\n",
        "\n",
        "With these two equations, we can plug them into the gradient descent formula to linear regression:\n",
        "\n",
        "$$w_{i+1} = w_i - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}(f_{w, b}(x^{(i)}) - y^{(i)})x^{(i)}$$\n",
        "\n",
        "$$b_{i+1} = b_i - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}(f_{w, b}(x^{(i)}) - y^{(i)})$$\n",
        "\n",
        "**NOTE**: w and b have to be updated simultaneously!\n",
        "\n",
        "It's important to choose a convex cost function, ,which means that this function does not have local minimas, only a global one. So, if you choose a right $\\alpha$ parameter (learning rate), your model will always and easily fall into the global minima when implementing gradient descent algorithm."
      ],
      "metadata": {
        "id": "EqAEKX7W993l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixa2xLyqTd7u"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}