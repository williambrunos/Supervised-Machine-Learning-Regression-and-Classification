{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gradient_descent.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/williambrunos/Supervised-Machine-Learning-Regression-and-Classification/blob/main/Week_1/Regression/gradient_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model with gradient descent \n",
        "\n",
        "In the last class (**regression_model**), we learnt about how the model can adjust the parameters w and b in order to improve the accuracy of the predictions minimizing the cost function $J(w, b)$. The main technique used to do so is called the **gradient descent** method. \n",
        "\n",
        "**obs**: it's imnportant to notice that we are working with a cost function which values depends only of two paramters and we are going to use the gradient descent algorithm. Turns out that we can apply this algorithm to minimize any function according to its parameters, for instance: $J(w_0, w_1, w_2...w_n, b)$\n",
        "\n",
        "Outline:\n",
        "1. Start with random values for w and b;\n",
        "2. Keep changing w and b in order to reduce J(w, b);\n",
        "3. Until we settle at or near a minimun;\n",
        "\n",
        "We have to take care of the local minimas!\n",
        "\n",
        "Beggining with random values for w and b, the algorithm looks foward for the steepest descent on the cost function surface in the features space and tooks a baby step on that direction. The direction taken by the algorithm at some point is the opposite of that indicated by the gradient of the function at that same point.\n",
        "\n",
        "Note: The gradient of the function at some point is a vector which indicates the direction of the highest growing of the function.\n",
        "\n",
        "Take a look at these images to visualize better this algorithm:\n",
        "\n",
        "$$figure 1 - 2D gradient descent$$\n",
        "![2D gradient descent](https://miro.medium.com/max/1142/1*AZzu43KoxDamVpWMVW0zfw.png)\n",
        "$$refference - Mediun$$\n",
        "\n",
        "$$figure 2 - 3D gradient descent$$\n",
        "![3D gradient descent](https://miro.medium.com/proxy/1*f9a162GhpMbiTVTAua_lLQ.png)\n",
        "$$refference - Mediun$$\n"
      ],
      "metadata": {
        "id": "z8EeGlt8Teoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing Gradient Descent\n",
        "\n",
        "Repeat these steps until convergence:\n",
        "\n",
        "$$w_{i+1} = w_i - \\alpha \\frac{\\partial}{\\partial{w}} J(w_i, b_i)$$\n",
        "$$b_{i+1} = b_i - \\alpha \\frac{\\partial}{\\partial{w}} J(w_i, b_i)$$\n",
        "\n",
        "- $\\alpha \\in [0, 1]$: **learning rate**, controls how big is going to be every step at each iteration of the algorithm;\n",
        "- $\\frac{\\partial}{\\partial{w}} J(w_i, b_i)$: sets the direction of the steepest step taken by the algorithm;\n",
        "\n",
        "**Note**: The updates of w and b must be **SIMULTANEOUS**"
      ],
      "metadata": {
        "id": "hkP-vOuPewhE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixa2xLyqTd7u"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}